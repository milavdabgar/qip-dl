{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d491ceb6",
      "metadata": {},
      "source": [
        "# Data Cleaning Exercise - Laboratory 2\n",
        "\n",
        "**AICTE Faculty ID:** 1-3241967546  \n",
        "**Faculty Name:** Milav Jayeshkuamar Dabgar  \n",
        "**Date:** July 16, 2025\n",
        "\n",
        "---\n",
        "\n",
        "## Objective\n",
        "This laboratory exercise focuses on data cleaning and preprocessing techniques using the Car Evaluation dataset. We will perform comprehensive data analysis, handle missing values, feature engineering, and prepare the dataset for machine learning applications."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4791aa8",
      "metadata": {
        "id": "a4791aa8",
        "papermill": {
          "duration": 0.025062,
          "end_time": "2024-08-16T18:34:29.658771",
          "exception": false,
          "start_time": "2024-08-16T18:34:29.633709",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "## Import Libraries\n",
        "\n",
        "*   numpy\n",
        "*   matplotlib.pyplot\n",
        "*   pandas\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "9a4c02c9",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries for data manipulation and visualization\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "print(\"Libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28b0c0fd",
      "metadata": {
        "id": "28b0c0fd",
        "papermill": {
          "duration": 0.0253,
          "end_time": "2024-08-16T18:34:32.810872",
          "exception": false,
          "start_time": "2024-08-16T18:34:32.785572",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "## Import Data Set\n",
        "\n",
        "*   car_evaluation.csv\n",
        "*   see the given dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "86220705",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset loaded successfully!\n",
            "Dataset shape: (1728, 7)\n",
            "\n",
            "First few rows of the raw dataset:\n",
            "       0      1  2  3      4     5      6\n",
            "0  vhigh  vhigh  2  2  small   low  unacc\n",
            "1  vhigh  vhigh  2  2  small   med  unacc\n",
            "2  vhigh  vhigh  2  2  small  high  unacc\n",
            "3  vhigh  vhigh  2  2    med   low  unacc\n",
            "4  vhigh  vhigh  2  2    med   med  unacc\n",
            "\n",
            "Last few rows to check the data:\n",
            "        0    1      2     3    4     5      6\n",
            "1723  low  low  5more  more  med   med   good\n",
            "1724  low  low  5more  more  med  high  vgood\n",
            "1725  low  low  5more  more  big   low  unacc\n",
            "1726  low  low  5more  more  big   med   good\n",
            "1727  low  low  5more  more  big  high  vgood\n"
          ]
        }
      ],
      "source": [
        "# Load the car evaluation dataset\n",
        "# Note: The CSV file doesn't have headers, so we'll add them manually\n",
        "data = pd.read_csv('/Users/milav/Code/qip-dl/ml/day-2/car_evaluation.csv', header=None)\n",
        "\n",
        "print(\"Dataset loaded successfully!\")\n",
        "print(f\"Dataset shape: {data.shape}\")\n",
        "print(\"\\nFirst few rows of the raw dataset:\")\n",
        "print(data.head())\n",
        "\n",
        "print(\"\\nLast few rows to check the data:\")\n",
        "print(data.tail())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6fd317a",
      "metadata": {
        "id": "f6fd317a",
        "papermill": {
          "duration": 0.026519,
          "end_time": "2024-08-16T18:34:32.944058",
          "exception": false,
          "start_time": "2024-08-16T18:34:32.917539",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "## data analysis\n",
        "\n",
        "*   See the number of rows and column\n",
        "*   Need to change the column names-- rename them\n",
        "*   See the dataset after adding new column names\n",
        "*   In each column/features, see the distribution of every catorical values\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "67691dce",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== DATASET DIMENSIONS ===\n",
            "Number of rows: 1728\n",
            "Number of columns: 7\n",
            "\n",
            "=== DATASET AFTER ADDING COLUMN NAMES ===\n",
            "  buying_price maintenance_cost doors persons luggage_boot safety class_value\n",
            "0        vhigh            vhigh     2       2        small    low       unacc\n",
            "1        vhigh            vhigh     2       2        small    med       unacc\n",
            "2        vhigh            vhigh     2       2        small   high       unacc\n",
            "3        vhigh            vhigh     2       2          med    low       unacc\n",
            "4        vhigh            vhigh     2       2          med    med       unacc\n",
            "5        vhigh            vhigh     2       2          med   high       unacc\n",
            "6        vhigh            vhigh     2       2          big    low       unacc\n",
            "7        vhigh            vhigh     2       2          big    med       unacc\n",
            "8        vhigh            vhigh     2       2          big   high       unacc\n",
            "9        vhigh            vhigh     2       4        small    low       unacc\n",
            "\n",
            "=== BASIC DATASET INFO ===\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1728 entries, 0 to 1727\n",
            "Data columns (total 7 columns):\n",
            " #   Column            Non-Null Count  Dtype \n",
            "---  ------            --------------  ----- \n",
            " 0   buying_price      1728 non-null   object\n",
            " 1   maintenance_cost  1728 non-null   object\n",
            " 2   doors             1728 non-null   object\n",
            " 3   persons           1728 non-null   object\n",
            " 4   luggage_boot      1728 non-null   object\n",
            " 5   safety            1728 non-null   object\n",
            " 6   class_value       1728 non-null   object\n",
            "dtypes: object(7)\n",
            "memory usage: 94.6+ KB\n",
            "None\n",
            "\n",
            "=== DATASET DESCRIPTION ===\n",
            "       buying_price maintenance_cost doors persons luggage_boot safety  \\\n",
            "count          1728             1728  1728    1728         1728   1728   \n",
            "unique            4                4     4       3            3      3   \n",
            "top           vhigh            vhigh     2       2        small    low   \n",
            "freq            432              432   432     576          576    576   \n",
            "\n",
            "       class_value  \n",
            "count         1728  \n",
            "unique           4  \n",
            "top          unacc  \n",
            "freq          1210  \n"
          ]
        }
      ],
      "source": [
        "# Data Analysis Section\n",
        "\n",
        "# Check the number of rows and columns\n",
        "print(\"=== DATASET DIMENSIONS ===\")\n",
        "print(f\"Number of rows: {data.shape[0]}\")\n",
        "print(f\"Number of columns: {data.shape[1]}\")\n",
        "\n",
        "# Add proper column names based on the car evaluation dataset documentation\n",
        "column_names = ['buying_price', 'maintenance_cost', 'doors', 'persons', 'luggage_boot', 'safety', 'class_value']\n",
        "data.columns = column_names\n",
        "\n",
        "print(\"\\n=== DATASET AFTER ADDING COLUMN NAMES ===\")\n",
        "print(data.head(10))\n",
        "\n",
        "print(\"\\n=== BASIC DATASET INFO ===\")\n",
        "print(data.info())\n",
        "\n",
        "print(\"\\n=== DATASET DESCRIPTION ===\")\n",
        "print(data.describe(include='all'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "7fc677d7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== DISTRIBUTION OF CATEGORICAL VALUES ===\n",
            "\n",
            "--- BUYING_PRICE ---\n",
            "buying_price\n",
            "vhigh    432\n",
            "high     432\n",
            "med      432\n",
            "low      432\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Percentages:\n",
            "  vhigh: 432 (25.0%)\n",
            "  high: 432 (25.0%)\n",
            "  med: 432 (25.0%)\n",
            "  low: 432 (25.0%)\n",
            "----------------------------------------\n",
            "\n",
            "--- MAINTENANCE_COST ---\n",
            "maintenance_cost\n",
            "vhigh    432\n",
            "high     432\n",
            "med      432\n",
            "low      432\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Percentages:\n",
            "  vhigh: 432 (25.0%)\n",
            "  high: 432 (25.0%)\n",
            "  med: 432 (25.0%)\n",
            "  low: 432 (25.0%)\n",
            "----------------------------------------\n",
            "\n",
            "--- DOORS ---\n",
            "doors\n",
            "2        432\n",
            "3        432\n",
            "4        432\n",
            "5more    432\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Percentages:\n",
            "  2: 432 (25.0%)\n",
            "  3: 432 (25.0%)\n",
            "  4: 432 (25.0%)\n",
            "  5more: 432 (25.0%)\n",
            "----------------------------------------\n",
            "\n",
            "--- PERSONS ---\n",
            "persons\n",
            "2       576\n",
            "4       576\n",
            "more    576\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Percentages:\n",
            "  2: 576 (33.33%)\n",
            "  4: 576 (33.33%)\n",
            "  more: 576 (33.33%)\n",
            "----------------------------------------\n",
            "\n",
            "--- LUGGAGE_BOOT ---\n",
            "luggage_boot\n",
            "small    576\n",
            "med      576\n",
            "big      576\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Percentages:\n",
            "  small: 576 (33.33%)\n",
            "  med: 576 (33.33%)\n",
            "  big: 576 (33.33%)\n",
            "----------------------------------------\n",
            "\n",
            "--- SAFETY ---\n",
            "safety\n",
            "low     576\n",
            "med     576\n",
            "high    576\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Percentages:\n",
            "  low: 576 (33.33%)\n",
            "  med: 576 (33.33%)\n",
            "  high: 576 (33.33%)\n",
            "----------------------------------------\n",
            "\n",
            "--- CLASS_VALUE ---\n",
            "class_value\n",
            "unacc    1210\n",
            "acc       384\n",
            "good       69\n",
            "vgood      65\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Percentages:\n",
            "  unacc: 1210 (70.02%)\n",
            "  acc: 384 (22.22%)\n",
            "  good: 69 (3.99%)\n",
            "  vgood: 65 (3.76%)\n",
            "----------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Analyze the distribution of categorical values in each column\n",
        "print(\"=== DISTRIBUTION OF CATEGORICAL VALUES ===\")\n",
        "\n",
        "for column in data.columns:\n",
        "    print(f\"\\n--- {column.upper()} ---\")\n",
        "    value_counts = data[column].value_counts()\n",
        "    print(value_counts)\n",
        "    \n",
        "    # Calculate percentages\n",
        "    percentages = (data[column].value_counts(normalize=True) * 100).round(2)\n",
        "    print(f\"\\nPercentages:\")\n",
        "    for value, count in value_counts.items():\n",
        "        percentage = percentages[value]\n",
        "        print(f\"  {value}: {count} ({percentage}%)\")\n",
        "    print(\"-\" * 40)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87123435",
      "metadata": {
        "id": "87123435",
        "papermill": {
          "duration": 0.026285,
          "end_time": "2024-08-16T18:34:33.772838",
          "exception": false,
          "start_time": "2024-08-16T18:34:33.746553",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "### Checking missing values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "326a0123",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== MISSING VALUES ANALYSIS ===\n",
            "Null values per column:\n",
            "buying_price        0\n",
            "maintenance_cost    0\n",
            "doors               0\n",
            "persons             0\n",
            "luggage_boot        0\n",
            "safety              0\n",
            "class_value         0\n",
            "dtype: int64\n",
            "\n",
            "Empty strings per column:\n",
            "buying_price        0\n",
            "maintenance_cost    0\n",
            "doors               0\n",
            "persons             0\n",
            "luggage_boot        0\n",
            "safety              0\n",
            "class_value         0\n",
            "dtype: int64\n",
            "\n",
            "Whitespace-only values per column:\n",
            "buying_price        0\n",
            "maintenance_cost    0\n",
            "doors               0\n",
            "persons             0\n",
            "luggage_boot        0\n",
            "safety              0\n",
            "class_value         0\n",
            "dtype: int64\n",
            "\n",
            "Total missing values in dataset: 0\n",
            "✅ Great! No missing values found in the dataset.\n"
          ]
        }
      ],
      "source": [
        "# Check for missing values in the dataset\n",
        "print(\"=== MISSING VALUES ANALYSIS ===\")\n",
        "\n",
        "# Check for null values\n",
        "missing_values = data.isnull().sum()\n",
        "print(\"Null values per column:\")\n",
        "print(missing_values)\n",
        "\n",
        "# Check for empty strings\n",
        "empty_strings = (data == '').sum()\n",
        "print(\"\\nEmpty strings per column:\")\n",
        "print(empty_strings)\n",
        "\n",
        "# Check for whitespace-only values\n",
        "whitespace_only = data.apply(lambda x: x.astype(str).str.strip().eq('').sum())\n",
        "print(\"\\nWhitespace-only values per column:\")\n",
        "print(whitespace_only)\n",
        "\n",
        "# Total missing data\n",
        "total_missing = missing_values.sum() + empty_strings.sum() + whitespace_only.sum()\n",
        "print(f\"\\nTotal missing values in dataset: {total_missing}\")\n",
        "\n",
        "if total_missing == 0:\n",
        "    print(\"✅ Great! No missing values found in the dataset.\")\n",
        "else:\n",
        "    print(\"⚠️ Missing values detected and need to be handled.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f409516",
      "metadata": {
        "id": "8f409516",
        "papermill": {
          "duration": 0.026783,
          "end_time": "2024-08-16T18:34:33.951820",
          "exception": false,
          "start_time": "2024-08-16T18:34:33.925037",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "##  Feature Engineering\n",
        "\n",
        "*   Convert Data type of columns (doors , persons)\n",
        "*   Encode the non numerical value to numerical values\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "66a69c8c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== ORIGINAL DATA TYPES ===\n",
            "buying_price        object\n",
            "maintenance_cost    object\n",
            "doors               object\n",
            "persons             object\n",
            "luggage_boot        object\n",
            "safety              object\n",
            "class_value         object\n",
            "dtype: object\n",
            "\n",
            "=== CONVERTING DATA TYPES ===\n",
            "Converting 'doors' column...\n",
            "Doors unique values after conversion: [np.int64(2), np.int64(3), np.int64(4), np.int64(5)]\n",
            "Converting 'persons' column...\n",
            "Persons unique values after conversion: [np.int64(2), np.int64(4), np.int64(6)]\n",
            "\n",
            "=== DATA TYPES AFTER CONVERSION ===\n",
            "buying_price        object\n",
            "maintenance_cost    object\n",
            "doors                int64\n",
            "persons              int64\n",
            "luggage_boot        object\n",
            "safety              object\n",
            "class_value         object\n",
            "dtype: object\n"
          ]
        }
      ],
      "source": [
        "# Feature Engineering Section\n",
        "\n",
        "# Create a copy of the data for processing\n",
        "processed_data = data.copy()\n",
        "\n",
        "print(\"=== ORIGINAL DATA TYPES ===\")\n",
        "print(processed_data.dtypes)\n",
        "\n",
        "# Convert specific columns that should be numerical\n",
        "print(\"\\n=== CONVERTING DATA TYPES ===\")\n",
        "\n",
        "# Handle 'doors' column - convert to numeric\n",
        "print(\"Converting 'doors' column...\")\n",
        "doors_mapping = {'2': 2, '3': 3, '4': 4, '5more': 5}\n",
        "processed_data['doors'] = processed_data['doors'].map(doors_mapping)\n",
        "print(f\"Doors unique values after conversion: {sorted(processed_data['doors'].unique())}\")\n",
        "\n",
        "# Handle 'persons' column - convert to numeric  \n",
        "print(\"Converting 'persons' column...\")\n",
        "persons_mapping = {'2': 2, '4': 4, 'more': 6}  # Assuming 'more' means 6+ people\n",
        "processed_data['persons'] = processed_data['persons'].map(persons_mapping)\n",
        "print(f\"Persons unique values after conversion: {sorted(processed_data['persons'].unique())}\")\n",
        "\n",
        "print(\"\\n=== DATA TYPES AFTER CONVERSION ===\")\n",
        "print(processed_data.dtypes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "f8ca63b2",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== ENCODING CATEGORICAL VARIABLES ===\n",
            "\n",
            "Encoding buying_price...\n",
            "Original values: ['vhigh', 'high', 'med', 'low']\n",
            "Encoded values: [np.int64(0), np.int64(1), np.int64(2), np.int64(3)]\n",
            "Mapping: {'high': np.int64(0), 'low': np.int64(1), 'med': np.int64(2), 'vhigh': np.int64(3)}\n",
            "\n",
            "Encoding maintenance_cost...\n",
            "Original values: ['vhigh', 'high', 'med', 'low']\n",
            "Encoded values: [np.int64(0), np.int64(1), np.int64(2), np.int64(3)]\n",
            "Mapping: {'high': np.int64(0), 'low': np.int64(1), 'med': np.int64(2), 'vhigh': np.int64(3)}\n",
            "\n",
            "Encoding luggage_boot...\n",
            "Original values: ['small', 'med', 'big']\n",
            "Encoded values: [np.int64(0), np.int64(1), np.int64(2)]\n",
            "Mapping: {'big': np.int64(0), 'med': np.int64(1), 'small': np.int64(2)}\n",
            "\n",
            "Encoding safety...\n",
            "Original values: ['low', 'med', 'high']\n",
            "Encoded values: [np.int64(0), np.int64(1), np.int64(2)]\n",
            "Mapping: {'high': np.int64(0), 'low': np.int64(1), 'med': np.int64(2)}\n",
            "\n",
            "Encoding class_value...\n",
            "Original values: ['unacc', 'acc', 'vgood', 'good']\n",
            "Encoded values: [np.int64(0), np.int64(1), np.int64(2), np.int64(3)]\n",
            "Mapping: {'acc': np.int64(0), 'good': np.int64(1), 'unacc': np.int64(2), 'vgood': np.int64(3)}\n",
            "\n",
            "=== ENCODED DATASET SAMPLE ===\n",
            "   buying_price  maintenance_cost  doors  persons  luggage_boot  safety  \\\n",
            "0             3                 3      2        2             2       1   \n",
            "1             3                 3      2        2             2       2   \n",
            "2             3                 3      2        2             2       0   \n",
            "3             3                 3      2        2             1       1   \n",
            "4             3                 3      2        2             1       2   \n",
            "5             3                 3      2        2             1       0   \n",
            "6             3                 3      2        2             0       1   \n",
            "7             3                 3      2        2             0       2   \n",
            "8             3                 3      2        2             0       0   \n",
            "9             3                 3      2        4             2       1   \n",
            "\n",
            "   class_value  \n",
            "0            2  \n",
            "1            2  \n",
            "2            2  \n",
            "3            2  \n",
            "4            2  \n",
            "5            2  \n",
            "6            2  \n",
            "7            2  \n",
            "8            2  \n",
            "9            2  \n"
          ]
        }
      ],
      "source": [
        "# Encode categorical variables to numerical values\n",
        "print(\"=== ENCODING CATEGORICAL VARIABLES ===\")\n",
        "\n",
        "# Columns that need encoding (excluding doors and persons which are now numeric)\n",
        "categorical_columns = ['buying_price', 'maintenance_cost', 'luggage_boot', 'safety', 'class_value']\n",
        "\n",
        "# Store the original values for reference\n",
        "original_mappings = {}\n",
        "\n",
        "# Initialize label encoders\n",
        "label_encoders = {}\n",
        "\n",
        "for column in categorical_columns:\n",
        "    print(f\"\\nEncoding {column}...\")\n",
        "    \n",
        "    # Store original values\n",
        "    original_values = processed_data[column].unique()\n",
        "    original_mappings[column] = original_values\n",
        "    print(f\"Original values: {list(original_values)}\")\n",
        "    \n",
        "    # Apply label encoding\n",
        "    le = LabelEncoder()\n",
        "    processed_data[column] = le.fit_transform(processed_data[column])\n",
        "    label_encoders[column] = le\n",
        "    \n",
        "    # Show the mapping\n",
        "    encoded_values = processed_data[column].unique()\n",
        "    print(f\"Encoded values: {sorted(encoded_values)}\")\n",
        "    \n",
        "    # Create mapping dictionary for clarity\n",
        "    mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
        "    print(f\"Mapping: {mapping}\")\n",
        "\n",
        "print(\"\\n=== ENCODED DATASET SAMPLE ===\")\n",
        "print(processed_data.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "513f4c74",
      "metadata": {
        "id": "513f4c74",
        "papermill": {
          "duration": 0.027628,
          "end_time": "2024-08-16T18:34:34.730958",
          "exception": false,
          "start_time": "2024-08-16T18:34:34.703330",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "## Seperate dependent and independent variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "a36cd511",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== SEPARATING FEATURES AND TARGET ===\n",
            "Independent variables (Features):\n",
            "Feature columns: ['buying_price', 'maintenance_cost', 'doors', 'persons', 'luggage_boot', 'safety']\n",
            "Features shape: (1728, 6)\n",
            "\n",
            "Dependent variable (Target): 'class_value'\n",
            "Target shape: (1728,)\n",
            "\n",
            "=== FEATURE STATISTICS ===\n",
            "       buying_price  maintenance_cost        doors      persons  luggage_boot  \\\n",
            "count   1728.000000       1728.000000  1728.000000  1728.000000   1728.000000   \n",
            "mean       1.500000          1.500000     3.500000     4.000000      1.000000   \n",
            "std        1.118358          1.118358     1.118358     1.633466      0.816733   \n",
            "min        0.000000          0.000000     2.000000     2.000000      0.000000   \n",
            "25%        0.750000          0.750000     2.750000     2.000000      0.000000   \n",
            "50%        1.500000          1.500000     3.500000     4.000000      1.000000   \n",
            "75%        2.250000          2.250000     4.250000     6.000000      2.000000   \n",
            "max        3.000000          3.000000     5.000000     6.000000      2.000000   \n",
            "\n",
            "            safety  \n",
            "count  1728.000000  \n",
            "mean      1.000000  \n",
            "std       0.816733  \n",
            "min       0.000000  \n",
            "25%       0.000000  \n",
            "50%       1.000000  \n",
            "75%       2.000000  \n",
            "max       2.000000  \n",
            "\n",
            "=== TARGET VARIABLE DISTRIBUTION ===\n",
            "class_value\n",
            "0     384\n",
            "1      69\n",
            "2    1210\n",
            "3      65\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Target percentages:\n",
            "Class 0: 384 samples (22.22%)\n",
            "Class 1: 69 samples (3.99%)\n",
            "Class 2: 1210 samples (70.02%)\n",
            "Class 3: 65 samples (3.76%)\n",
            "\n",
            "Dataset is ready for machine learning!\n",
            "Total samples: 1728\n",
            "Total features: 6\n",
            "Target classes: 4\n"
          ]
        }
      ],
      "source": [
        "# Separate dependent and independent variables\n",
        "print(\"=== SEPARATING FEATURES AND TARGET ===\")\n",
        "\n",
        "# Independent variables (features) - all columns except the target\n",
        "X = processed_data.drop('class_value', axis=1)\n",
        "print(\"Independent variables (Features):\")\n",
        "print(f\"Feature columns: {list(X.columns)}\")\n",
        "print(f\"Features shape: {X.shape}\")\n",
        "\n",
        "# Dependent variable (target)\n",
        "y = processed_data['class_value']\n",
        "print(f\"\\nDependent variable (Target): 'class_value'\")\n",
        "print(f\"Target shape: {y.shape}\")\n",
        "\n",
        "# Show feature statistics\n",
        "print(\"\\n=== FEATURE STATISTICS ===\")\n",
        "print(X.describe())\n",
        "\n",
        "# Show target distribution\n",
        "print(\"\\n=== TARGET VARIABLE DISTRIBUTION ===\")\n",
        "target_distribution = y.value_counts().sort_index()\n",
        "print(target_distribution)\n",
        "\n",
        "# Show target percentages\n",
        "target_percentages = (y.value_counts(normalize=True) * 100).round(2).sort_index()\n",
        "print(\"\\nTarget percentages:\")\n",
        "for value, percentage in target_percentages.items():\n",
        "    count = target_distribution[value]\n",
        "    print(f\"Class {value}: {count} samples ({percentage}%)\")\n",
        "\n",
        "print(f\"\\nDataset is ready for machine learning!\")\n",
        "print(f\"Total samples: {X.shape[0]}\")\n",
        "print(f\"Total features: {X.shape[1]}\")\n",
        "print(f\"Target classes: {len(y.unique())}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8debfe7",
      "metadata": {
        "id": "f8debfe7",
        "papermill": {
          "duration": 0.030549,
          "end_time": "2024-08-16T18:34:34.867335",
          "exception": false,
          "start_time": "2024-08-16T18:34:34.836786",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "## Split the data into training and test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "80f66031",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== SPLITTING DATA INTO TRAIN AND TEST SETS ===\n",
            "Data split completed successfully!\n",
            "\n",
            "Training set:\n",
            "  X_train shape: (1382, 6)\n",
            "  y_train shape: (1382,)\n",
            "\n",
            "Testing set:\n",
            "  X_test shape: (346, 6)\n",
            "  y_test shape: (346,)\n",
            "\n",
            "Split ratios:\n",
            "  Training: 80.0%\n",
            "  Testing: 20.0%\n",
            "\n",
            "=== CLASS DISTRIBUTION AFTER SPLIT ===\n",
            "Training set class distribution:\n",
            "  Class 0: 307 samples (22.2%)\n",
            "  Class 1: 55 samples (4.0%)\n",
            "  Class 2: 968 samples (70.0%)\n",
            "  Class 3: 52 samples (3.8%)\n",
            "\n",
            "Testing set class distribution:\n",
            "  Class 0: 77 samples (22.3%)\n",
            "  Class 1: 14 samples (4.0%)\n",
            "  Class 2: 242 samples (69.9%)\n",
            "  Class 3: 13 samples (3.8%)\n",
            "\n",
            "✅ Data preprocessing completed successfully!\n",
            "The dataset is now ready for machine learning algorithms.\n"
          ]
        }
      ],
      "source": [
        "# Split the data into training and testing sets\n",
        "print(\"=== SPLITTING DATA INTO TRAIN AND TEST SETS ===\")\n",
        "\n",
        "# Perform the split (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, \n",
        "    test_size=0.2, \n",
        "    random_state=42, \n",
        "    stratify=y  # Ensure balanced split across all classes\n",
        ")\n",
        "\n",
        "print(\"Data split completed successfully!\")\n",
        "print(f\"\\nTraining set:\")\n",
        "print(f\"  X_train shape: {X_train.shape}\")\n",
        "print(f\"  y_train shape: {y_train.shape}\")\n",
        "\n",
        "print(f\"\\nTesting set:\")\n",
        "print(f\"  X_test shape: {X_test.shape}\")\n",
        "print(f\"  y_test shape: {y_test.shape}\")\n",
        "\n",
        "# Verify the split ratios\n",
        "print(f\"\\nSplit ratios:\")\n",
        "print(f\"  Training: {len(X_train)/len(X)*100:.1f}%\")\n",
        "print(f\"  Testing: {len(X_test)/len(X)*100:.1f}%\")\n",
        "\n",
        "# Check class distribution in training and testing sets\n",
        "print(f\"\\n=== CLASS DISTRIBUTION AFTER SPLIT ===\")\n",
        "print(\"Training set class distribution:\")\n",
        "train_dist = y_train.value_counts().sort_index()\n",
        "for class_val, count in train_dist.items():\n",
        "    percentage = (count / len(y_train) * 100)\n",
        "    print(f\"  Class {class_val}: {count} samples ({percentage:.1f}%)\")\n",
        "\n",
        "print(\"\\nTesting set class distribution:\")\n",
        "test_dist = y_test.value_counts().sort_index()\n",
        "for class_val, count in test_dist.items():\n",
        "    percentage = (count / len(y_test) * 100)\n",
        "    print(f\"  Class {class_val}: {count} samples ({percentage:.1f}%)\")\n",
        "\n",
        "print(\"\\n✅ Data preprocessing completed successfully!\")\n",
        "print(\"The dataset is now ready for machine learning algorithms.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "datasetId": 2298,
          "sourceId": 3884,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30746,
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 13.89258,
      "end_time": "2024-08-16T18:34:39.073033",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2024-08-16T18:34:25.180453",
      "version": "2.5.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
